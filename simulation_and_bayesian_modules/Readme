About simulation:

The images used for simulation is download from http://www.image-net.org/challenges/LSVRC/ and http://sketchy.eye.gatech.edu/. Image-Net provides the natural images (RGB-color) and sketchy provides sketch images (gray-color).

After comply the super-resolution source, there will results in several exes (the comply of super-resolution source need cmake, gcc, g++ and opencv2.4 lib). 

To compile the code, one should:

cd ./simulation_and_bayesian_modules
mkdir build
cd ./build
cmake ..
make -j 2

A "bin" dir will be generated and the following exes will be created: calib  lapls  montage  png2pt  ranpath  sravg  srsimu  subtract

--------------------------------------------------------------------------------------------------------------------

There are three exes to do simulation. 
 lapls: input an color image and output a gray version of the laplace filtered color image (6x), for example:
 
./lapls -i 0037.png -o test.pgm -s 480 -m 0
 
Here, test.pgm is a generated gray image (-m to say which part for translation, 0 center, 1 2 3 4 four corner).


 ranpath: input a sketch image and output a size adjusted gray image.
 
./ranpath -i ./n02694662_17579-5.png -o sketch.pgm -s 480

Here, sketch.pgm is a rescaled sketch image (gray image).

 
 srsimu: input a high resolution gray image (fluorophore distribution) and output a series of low resolution images (here the log-norm and transmitting parameters are fixed, but easy to be changed in code). srsimu uses a fully random parameter initialization.

./srsimu -i test.pgm -o test -n 200 -s 0.125 -p 8

Here, -n (number of images), -s (downsampling scale) -p (used to adjust psf width, 8 is in default).

Using these three exes, we can generate simulation data for deep learning training (in different binned size).

We totally generated about 4000 datasets from imagenet and 8000 datasets from sketchy. These simulated datasets are used for deep learning training.

--------------------------------------------------------------------------------------------------------------------
The current neural network is designed for 60x60 low-resolution fluorescent images with 200 frames (these can be changed by re-training).  
--------------------------------------------------------------------------------------------------------------------

There are several exes for the process of real data. The deep neural network need inputs of a set of image frames (not tif format). Here, use the cell6G_ori as an example.

calib: crop a certain area and do Gaussian high pass filter, for example:

./calib -i cell6G_ori -o cellG_60_60_60_60 -s 3 -r 60,60,60,60

./calib -i cell6G_ori -o cellG_GAUSS -s 3

The input is the cell1G_ori directory (containing 200 image frames); The output is the cellG_60_60_60_60 (containing 200 high-passed image frames with Gaussian sigma=3) and the region is 60,60,60,60  (X,Y,WIDTH,HEIGHT). If not assigned the region, it is the whole image.

Then, the cellG_60_60_60_60 can be inputted to the trained neural network to generate an initial guess of the fluorophore distribution, which can be used further refined by Bayesian technique.

clip.sh: change a series of large-field image frames to a set of series of image frames with small patches.

"clip" dir will contain 0-0-60-60 0-40-60-60... subdirs. Each of the subdirs will contains a 200 frames of images with 60x60 pixels. cellG_clip is the input of neural network for large-field datasets.

montage: interpret the output of deep neural network to a large-field image.

./montage -i cell6-dp-output -o cell6-montage.pgm

cell6-montage.pgm is the primitive super-resolution result generated by deep neural work. 

png2pt: interpret the deep learning output an fluorophore positions.

./png2pt -i cell6-montage.pgm -o cell6-init-positions.txt

cell6-montage.pgm is the montage result from deep neural network.

*********************************************************************************************************************
Thus, this source includes the simulation and input / output operation for deep learning module. 

